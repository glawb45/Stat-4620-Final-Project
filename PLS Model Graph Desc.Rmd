---
title: "Untitled"
author: "Aidan Dilsavor"
date: "2024-11-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
train <- read.csv("C:/Users/aidan/Downloads/train.csv")
library(pls)
set.seed(30)
```

## PLS Model

The PLS Model uses Ames predictors and identifies new features $Z_1 , . . . , Z_M$ which are linear
combinations of the predictors $X_1 , . . . , X_P$ 

Then, it fits the regression model:

$y_i = \theta_0 + \sum_{m=1}^{M} \theta_m z_{mi} + \epsilon_i$



PLS models make assumptions such as:

Linearity: The relationship between predictors and the target variable is linear.

Independent Errors: Residuals are independent of each other.

Normality of Errors: Residuals are normally distributed.



```{r}

cols_to_update <- c(4, 7, 27, 73, 74, 75)

train[ , cols_to_update] <- lapply(train[ , cols_to_update], function(x) ifelse(is.na(x), 0, x))

mean_col_60 <- mean(train[, 60], na.rm = TRUE)

train[, 60][is.na(train[, 60])] <- mean_col_60

train[is.na(train)] <- "N/A"
```


```{r}
library(fastDummies)
library(caret)
train_data_dummies <- fastDummies::dummy_cols(train, remove_first_dummy = TRUE)

nzv <- nearZeroVar(train_data_dummies)
train_clean <- train_data_dummies[, -nzv]


train_clean <- train_clean[, -1]


categorical_cols_1_81 <- sapply(train_clean[, 1:81], function(x) is.factor(x) || is.character(x))


train_clean <- train_clean[, !(names(train_clean) %in% names(train_clean[, 1:81])[categorical_cols_1_81])]
```

Above, similar cleaning techniques are used, and an added step of removing 
near-zero variances is utilized to provide a more accurate PLS model

```{r}

pls_model <- plsr(SalePrice ~ ., data = train_clean, scale = TRUE, validation = "CV")

summary(pls_model)


```


```{r}

press_values <- pls_model$validation$PRESS

n <- nrow(train_clean)  
rmsep_values <- sqrt(press_values / n)

rmsep_subset <- rmsep_values[3:50]

plot(3:50, rmsep_subset, type = "b", 
     xlab = "Number of Components", ylab = "RMSEP", 
     main = "RMSEP vs Number of Components (3 and onward)",
     col = "blue", pch = 16)
```

The plot above highlights the ideal number of linear combination components using RMSEP. In this case, n = 5-7 appears to be the ideal number of components for the PLS model, which give us an RMSEP of about 33,700.
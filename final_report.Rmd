---
title: "STAT 4620 Project Report"
author: "Aidan Dilsavor, Jacob Bailey, Gaurav Law"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(dplyr)
library(skimr)
library(ggplot2)
library(reshape2)
library(tree)
library(randomForest)
library(tidyverse)
```

# Part 1: Exploratory Data Analysis

For this project, we were given a dataset containing features of houses in Ames, Iowa, USA. We were given a training set with 14600 observations and a testing set with 1447 observations. There are 79 different attributes - 43 categorical, and 36 numerical. The response variable is SalePrice, which is the numeric value in dollars that the house sold for, and this is what we are trying to predict with a model.

```{r}
# read data in
train_data = read.csv('train.csv')
test_data = read.csv('test.csv')

# combine data for transformations
combined_data = rbind(train_data, test_data)

num_training = 1460

# drop Id
combined_data['Id'] = NULL

# summary of data
skim(combined_data)

set.seed(1)
```

After initially examining the dataset, we can see there are many 'null' categorical values. However, many of them shown as 'null' are in fact representing a category such as 'None' for a given categorical predictor. Values such as these will be transformed to more meaningful, non-null categories below. For example if 'GarageQual' is null, it will become 'NoGarage'.

```{r}
# Replace NA in Alley with "NoAccess"
combined_data$Alley[is.na(combined_data$Alley)] = "NoAccess"

# One missing value in 'Utilities', but supposed to be no nulls. We will drop it
combined_data = subset(combined_data, !is.na(Utilities))

# Replace MasVnrType nulls with 'None'
combined_data$MasVnrType[is.na(combined_data$MasVnrType)] = "None"

# Replace BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2  nulls with 'NoBasement'
combined_data$BsmtQual[is.na(combined_data$BsmtQual)] = "NoBasement"
combined_data$BsmtCond[is.na(combined_data$BsmtCond)] = "NoBasement"
combined_data$BsmtExposure[is.na(combined_data$BsmtExposure)] = "NoBasement"
combined_data$BsmtFinType1[is.na(combined_data$BsmtFinType1)] = "NoBasement"
combined_data$BsmtFinType2[is.na(combined_data$BsmtFinType2)] = "NoBasement"

# One missing value in 'Electrical', but supposed to be no nulls. We will drop it
combined_data = subset(combined_data, !is.na(Electrical))

# One missing value in 'KitchenQual', but supposed to be no nulls. We will drop it
combined_data = subset(combined_data, !is.na(KitchenQual))

# One missing value in 'Functional', but supposed to be no nulls. We will drop it
combined_data = subset(combined_data, !is.na(Functional))

# Replace FireplaceQu nulls with 'NoFireplace'
combined_data$FireplaceQu[is.na(combined_data$FireplaceQu)] = "NoFireplace"

# Replace GarageType, GarageFinish, GarageQual, GarageCond nulls with 'NoGarage'
combined_data$GarageType[is.na(combined_data$GarageType)] = "NoGarage"
combined_data$GarageFinish[is.na(combined_data$GarageFinish)] = "NoGarage"
combined_data$GarageQual[is.na(combined_data$GarageQual)] = "NoGarage"
combined_data$GarageCond[is.na(combined_data$GarageCond)] = "NoGarage"

# Replace PoolQC nulls with 'NoPool'
combined_data$PoolQC[is.na(combined_data$PoolQC)] = "NoPool"

# Replace Fence nulls with 'NoFence'
combined_data$Fence[is.na(combined_data$Fence)] = "NoFence"

# Replace MiscFeature nulls with 'None'
combined_data$MiscFeature[is.na(combined_data$MiscFeature)] = "None"

# Replace SaleType nulls with 'Other'
combined_data$SaleType[is.na(combined_data$SaleType)] = "Other"

```

There are not many numerical null values, but for the ones that are, we will replace with the median value of that column to prevent any skew of the predictor to influence this estimate.

```{r}
# Impute LotFrontage with the median
combined_data$LotFrontage[is.na(combined_data$LotFrontage)] = median(combined_data$LotFrontage, na.rm = TRUE)

# Impute MasVnrArea with the median
combined_data$MasVnrArea[is.na(combined_data$MasVnrArea)] = median(combined_data$MasVnrArea, na.rm = TRUE)

# Impute BsmtFullBath with the median
combined_data$BsmtFullBath[is.na(combined_data$BsmtFullBath)] = median(combined_data$BsmtFullBath, na.rm = TRUE)

# Impute BsmtHalfBath with the median
combined_data$BsmtHalfBath[is.na(combined_data$BsmtHalfBath)] = median(combined_data$BsmtHalfBath, na.rm = TRUE)

# Impute GarageYrBlt with the median
combined_data$GarageYrBlt[is.na(combined_data$GarageYrBlt)] = median(combined_data$GarageYrBlt, na.rm = TRUE)
```


Now we'll make sure the data is complete
```{r}
any(is.na(combined_data))

```

After examining the data, we expect there to be strong multi-collinearity. This is due to the fact that there are so many predictors and also intuitively, predictors such as ‘GarageArea’ and ‘GarageCars’ will likely be highly correlated. To further examine this, we will create a correlation heatmap displaying the linear relationship strengths between each predictor.

```{r}

# Get categorical predictors
categorical_cols = names(combined_data)[sapply(combined_data, is.character) | sapply(combined_data, is.factor)]

# Get numerical predictors
numeric_cols = names(combined_data)[sapply(combined_data, is.numeric)]

# One-hot encode categorical variables
data_encoded = as.data.frame(model.matrix(~ . - 1, data = combined_data[categorical_cols]))

# Combine numeric and encoded categorical variables
data_combined = cbind(combined_data[numeric_cols], data_encoded)

# Compute correlation matrix
cor_matrix = cor(data_combined, use = "pairwise.complete.obs")

# Melt correlation matrix
cor_melt = melt(cor_matrix)

# Generate heatmap
ggplot(cor_melt, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),  # Remove x-axis labels
    axis.text.y = element_blank()   # Remove y-axis labels
  ) +
  labs(title = "Correlation Heatmap", x = "Predictors", y = "Predictors", fill = "Correlation")
```


From the above plot, it is clear that many predictors are strongly linearly correlated. Because of this, we are interested in models that can handle multicollinearity. We will also try to address this problem before making any models.
  
## Addressing Multicollinearity

Before building the models, we are going to remove the highly correlated predictors by examining correlations between all features.

```{r}
# Define threshold where predictors will be removed
threshold = 0.85

# Model matrix to get levels for categorical predictors
full_data_X = model.matrix(SalePrice ~ ., data = combined_data)[,-1]
full_data_Y = combined_data$SalePrice

# Compute the correlation matrix for the predictors 
correlation_matrix = cor(full_data_X, use = "complete.obs")

# Convert the correlation matrix to a data frame for filtering
cor_pairs = as.data.frame(as.table(correlation_matrix))

# Filter for significant correlations above a threshold (excluding self-correlations)
cor_pairs_filtered = subset(cor_pairs, abs(Freq) > threshold & Var1 != Var2)

# Sort by absolute correlation value
cor_pairs_filtered = cor_pairs_filtered[order(-abs(cor_pairs_filtered$Freq)), ]

# Display the top correlated predictors
print(cor_pairs_filtered)
```

From the above table there are a many pairs of variables with correlation above 0.85. This is very high, so we will remove one variable from each pair.

```{r}
# Columns to remove
columns_to_remove <- c("BsmtFinType1NoBasement", "GarageQualNoGarage", "GarageCondNoGarage", "GarageTypeNoGarage",  "BsmtFinType2NoBasement", "BsmtFinType1NoBasement", "BsmtQualNoBasement", "SaleConditionPartial", "Exterior2ndCmentBd", "Exterior2ndVinylSd", "BsmtExposureNoBasement", "Exterior2ndMetalSd", "MiscFeatureNone", "RoofStyleHip", "ExterQualTA", "FireplaceQuNoFireplace", "GarageCars", "Exterior2ndHdBoard", "ExterCondTA", "MSZoningFV", "Exterior2ndWd Sdng", "PoolQCNoPool")

full_data_X <- full_data_X[, !colnames(full_data_X) %in% columns_to_remove]

```

## Examining possible important features of the model

```{r}
# Compute correlations of each feature in the model matrix with SalePrice
correlations <- cor(full_data_X, full_data_Y, use = "complete.obs")

# Convert to a data frame
correlation_df <- data.frame(
  Feature = colnames(full_data_X),
  Correlation = correlations
)

# Sort by absolute correlation value in descending order
correlation_df <- correlation_df[order(-abs(correlation_df$Correlation)), ]

# Display the top correlations
print(head(correlation_df, 5))
```

From the above output, we can see before doing any modeling that likely important predictors are: "OverallQual", "GrLivArea", "GarageArea", "TotalBsmtSF", and "X1stFlrSF".

  
# Part 2: Model Analysis

For each model we wish to evaluate, we will fit it to the training data using cross validation for parameters, then validate the model using K-fold validation sets, and finally select the model with the lowest validated rmse to move on to testing.


### Ridge & LASSO

Ridge and LASSO regression are ideal for predicting SalePrice in the Ames housing dataset due to their ability to handle multicollinearity and prevent overfitting in datasets with many predictors. Ridge regression minimizes the impact of correlated features by shrinking coefficients, while LASSO performs variable selection by driving some coefficients to zero, creating simpler, more interpretable models.

**Ridge Description:**

Ridge regression minimizes the following cost function:

\[
\text{Loss} = \sum_{i=1}^n \left(y_i - \mathbf{x}_i^\top \beta \right)^2 + \lambda \sum_{j=1}^p \beta_j^2
\]

where:

- \(y_i\): Actual value of the target variable for observation \(i\).

- \(\mathbf{x}_i\): Predictor vector for observation \(i\).

- \(\beta\): Coefficient vector.

- \(\lambda\): Regularization parameter controlling the penalty on the sum of squared coefficients (\(\lambda > 0\)).

The second term penalizes large coefficients, shrinking them to reduce overfitting, but all predictors remain in the model.


**LASSO Description:**

Lasso regression minimizes the following cost function:

\[
\text{Loss} = \sum_{i=1}^n \left(y_i - \mathbf{x}_i^\top \beta \right)^2 + \lambda \sum_{j=1}^p |\beta_j|
\]

where:

- \(y_i\): Actual value of the target variable for observation \(i\).

- \(\mathbf{x}_i\): Predictor vector for observation \(i\).

- \(\beta\): Coefficient vector.

- \(\lambda\): Regularization parameter controlling the penalty on the absolute values of coefficients (\(\lambda > 0\)).

The \(L_1\)-norm penalty encourages sparsity in the model, driving some coefficients to zero and effectively selecting only the most relevant features.


**Model Validation:**


```{r}

# Split data into training and testing sets
trainX = full_data_X[1:num_training, ]
trainY = full_data_Y[1:num_training]

testX = full_data_X[(num_training + 1):nrow(full_data_X), ]
testY = full_data_Y[(num_training + 1):nrow(full_data_X)]

# 10-fold cross validation
cv_fit.lasso = cv.glmnet(trainX, trainY, alpha = 1, nfolds = 10) 
cv_fit.ridge = cv.glmnet(trainX, trainY, alpha = 0, nfolds = 10) 


# Optimal lambda
best_lambda_lasso = cv_fit.lasso$lambda.min
best_lambda_ridge = cv_fit.ridge$lambda.min


average_rmse_lasso = sqrt(cv_fit.lasso$cvm[cv_fit.lasso$lambda == best_lambda_lasso])
average_rmse_ridge = sqrt(cv_fit.ridge$cvm[cv_fit.ridge$lambda == best_lambda_ridge])

# Print results
print(paste("10-Fold Average Ridge RMSE: ", average_rmse_ridge))
print(paste("10-Fold Average Lasso RMSE: ", average_rmse_lasso))

```


**CV Lambda Plots for Ridge & LASSO**

The following plots show how the optimal regularization parameter $\lambda$ is chosen for LASSO and Ridge Regression.

```{r}

# Fit the Ridge model with cross-validation
ridge.cv = cv.glmnet(trainX, trainY, alpha=0)

# Fit the Lasso model with cross-validation
lasso.cv = cv.glmnet(trainX, trainY, alpha=1)

# plot lambdas
par(mfrow = c(1, 2), mar = c(5, 4, 4, 2) + 1) 

plot(ridge.cv,main='Ridge Lambda Performance') 
plot(lasso.cv, main='LASSO Lambda Performance') 


```

Ridge gives a lower validated RMSE than LASSO, so we will remove LASSO from consideration.

**Assumptions:**

Ridge Regression and PLS makes certain assumptions such as:

- Linearity: The relationship between predictors and the target variable is linear.

- Independent Errors: Errors are independent of each other.

- Normality of Errors: Errors are normally distributed.

- Homoscedasticity: The variances of the errors are equal.


To show these assumptions are satisfied, we will plot residuals of the model against fitted values, a histogram of the residuals, and a QQ plot.

**Ridge Plots**

```{r}
# Fit Ridge Regression
ridge_model = glmnet(trainX, trainY, alpha = 0)

# Predict on training data
predictions = predict(ridge_model, newx = trainX, s=ridge_model$lambda.min)

# Compute residuals
residuals = trainY - predictions

# Plotting

# Residuals vs Fitted
plot(predictions, residuals, xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")

# Histogram
hist(residuals, main = "Histogram of Residuals", xlab = "Residuals")

# QQ Plot
qqnorm(residuals)
qqline(residuals, col = "red")
```

We see here the trends from the QQ plots, as well as those comparing residuals to our fitted values. We want to minimize our residuals as much as possible, but these have a very large range, showing us that ridge might not be the best way to go when we begin our testing.

As with the QQ plots, we want the trend between our sample and predicted quantiles to be as close to zero as possible in order for normality and standardization to apply. While we see this does a good job in the middle of our model, perhaps using something like natural splines would have standardized this further.

## PLS Model

The PLS Model uses Ames predictors and identifies new features $Z_1 , . . . , Z_M$ which are linear
combinations of the predictors $X_1 , . . . , X_P$ 

Then, it fits the regression model:

$y_i = \theta_0 + \sum_{m=1}^{M} \theta_m z_{mi} + \epsilon_i$



PLS models make assumptions such as:

Linearity: The relationship between predictors and the target variable is linear.

Independent Errors: Residuals are independent of each other.

Normality of Errors: Residuals are normally distributed.


**PLS Plots**

```{r}
library(pls)

# Fit PLS
pls_model = plsr(trainY ~ trainX, ncomp = 7) 

# Predict on training data
predictions = predict(pls_model, ncomp = 7) 

# Compute residuals
residuals = trainY - predictions


# Plotting

# Residuals vs Fitted
plot(predictions, residuals, xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")

# Histogram
hist(residuals, main = "Histogram of Residuals", xlab = "Residuals")

# QQ Plot
qqnorm(residuals)
qqline(residuals, col = "red")
```


From the plots above, we see that neither model completely satisfies the necessary assumptions. When examining the fitted values vs residuals, it is clear that patterns exist and data points are not scattered randomly - especially when examining Ridge regression. While PLS gives better plots than Ridge for both the QQ and fitted vs residuals, they are still not optimal. We will explore a different model.

## Tree-based models: Decision Trees, Bagging, Random Forest

Now, we want to explore the impact of tree-based models on SalePrice to see any impact. The idea behind these types of models is making it almost "easier" to split based on decisions to be made in a tree.

### CART

With classification and regression trees, or CARTs, we identify a binary tree and split based on node purity. A couple ways to depict this are a dendrogram or a purity index plot, such as a Gini index. We want to have the least number of nodes that will still produce the best results for us. To do this, the CART algorithm recursively iterates through the tree to determine the most pure nodes and split there.

Splitting on just one node, or a CART having just one two children, will lead to overfitting — this is where cross-validation helps us, which we address in our model-making.

To grow a regression tree, we must split into a piecewise continuous function:
$R_1(j, s) = {X|X_j \leq s}$ and $R_2(j, s) = {X|X_j > s}$, where $X_j$ is our splitting variable and $s$ is our split-point.

We do not prune here so we will not focus on that.

Our loss function, somewhat similarly, that we will focus on relates to node purity — because we have talked about Gini in class, we will look at that loss function:
$$G = \sum_{k = 1}^K(\hat{p}_{mk}(1-\hat{p}_{mk})))$$

### Bagging

Bootstrap Aggregating, or Bagging, is a procedure to reduce variance (trees have a high variance). This approach also attempt to average the means of the trees recurisvely to smooth the discontinuity

In bagging, we first generate some amount B boostrapped samples, and fit those samples to our model:

$\hat{f}_{bag}(x)$ = $\frac{1}{B} \sum_{b=1}^B \hat{f}_b(x)$

The downside of bagging is it becomes much more difficult to interpret the tree.

### Random Forest

Random Forests are just a more refined version of bagging, the primary difference being the choice of the precitor subset size m. At each split, a random sample m features is drawn — the only features considered moving forward. What's nice about RFs is that each tree has the same expectation and the model method attempts to de-correlate the trees in order to reduce variance (this last point is why RF generally produces better results than bagging).

We attempt to stabilize the error rate and, like bagging, when the number of trees increases, overfitting is prevented. We generally choose to go with m = $\sqrt(p)$ in our model-making process.

### Tree-based Model-Making Using Training Data

```{r tree with training}

# redistribute the data into its original training and testing sets --> testing will be used later
training_data <- combined_data[1:num_training, ]
testing_data <- combined_data[num_training:nrow(combined_data), ]

# fit the CART model
mod_tree = tree(SalePrice ~ ., data = training_data)
mod_tree

# get model for decision tree
summary(mod_tree)

# plot the tree in a dendrogram
plot(mod_tree, cex = 0.8)
text(mod_tree, pretty = 0, cex = 0.6)

# Find RMSE
mod_tree_pred = predict(mod_tree, newdata = training_data)
mod_tree_rmse = sqrt(mean((training_data$SalePrice - mod_tree_pred)^2))
mod_tree_rmse

# Use K-fold cross validation to reduce tree complexity
cv_tree = cv.tree(mod_tree)
names(cv_tree)

cv_tree

# identify trends in deviance with respect to the size of the tree
with(cv_tree, plot(dev~size, type = 'b'))
```

```{r bagging with training}

set.seed(123)

# fit the bagging model
mod_bag = randomForest(SalePrice ~ ., data = training_data,  mtry = NCOL(training_data)-1, importance = TRUE)
mod_bag

# identify trends with error rate relative to the number of trees present
plot(mod_bag)

mod_bag_pred = predict(mod_bag, newdata = training_data)
mod_bag_rmse  = sqrt(mean((training_data$SalePrice - mod_bag_pred)^2))
mod_bag_rmse

# Make a variable importance plot to show how MSE changes and to show node purity --> higher node purity indicates a more "important" predictor solely from our training set
varImpPlot(mod_bag, cex = 0.6)
```

```{r Random Forest with training}

# fit the random forest model
mod_rf = randomForest(SalePrice ~ ., data = training_data,  mtry = sqrt(NCOL(training_data)-1), importance = TRUE)
mod_rf

plot(mod_rf)

mod_rf_pred = predict(mod_rf, newdata = training_data)
mod_rf_rmse  = sqrt(mean((training_data$SalePrice - mod_rf_pred)^2))
mod_rf_rmse

# run lines together to compare variable importance in bagging vs. RF
varImpPlot(mod_rf, cex = 0.6)
varImpPlot(mod_bag, cex = 0.6)

# Compare the three RMSEs to judge which one we will use for testing
cat("Tree RMSE:", mod_tree_rmse, "\n")
cat("Bagging RMSE:", mod_bag_rmse, "\n")
cat("Random Forest RMSE:", mod_rf_rmse)


```

Given these results, we will likely end up going with either a bagging or RF model as our final, as these two minimize the MSE most.

## Results with testing data

***Now: Use Lasso Model to Advantage***

Now that we have a pruned dataset from lasso, in terms of which variables are most important, we will use only those variables moving forward for optimal model selection. 

```{r final set manipulation}
final_set <- full_data_X
final_set <- as.data.frame(final_set) # convert the model matrix into a data frame

# Add SalePrice back to the dataset to make tree-based models
final_set <- cbind(SalePrice = combined_data$SalePrice, final_set)

print(coef(lasso.cv)) # use this line to help us determine the most significant variables

# Only keep informative predictors
final_set <- final_set[, c("SalePrice",
                           "MSSubClass",
                           "MSZoningRM",
                           "LotArea",
                           "NeighborhoodNoRidge",
                           "NeighborhoodNridgHt",
                           "NeighborhoodStoneBr",
                           "OverallQual",
                           "YearBuilt",
                           "YearRemodAdd",
                           "MasVnrArea",
                           "BsmtExposureGd",
                           "BsmtExposureNo",
                           "BsmtFinType1GLQ",
                           "BsmtFinSF1",
                           "TotalBsmtSF",
                           "X1stFlrSF",
                           "GrLivArea",
                           "KitchenQualTA",
                           "Fireplaces",
                           "GarageArea",
                           "WoodDeckSF",
                           "SaleTypeNew")]

ncol(final_set)

# same split used as above in ridge and lasso
training_data <- final_set[1:num_training, ]
testing_data <- final_set[num_training:nrow(final_set), ]

```

Now, we have our optimal dataset for model-making using our testing data. We only have 22 predictor variables now. Up until now with the trees, we have decided to not talk about the plots — now that we are using testing data, we will explain trends and such.


```{r tree with testing}

set.seed(123)
mod_tree = tree(SalePrice ~ ., data = training_data)
mod_tree

# get model for decision tree
summary(mod_tree)

# plot the tree
plot(mod_tree, cex = 0.8)   # Adjust tree size
text(mod_tree, pretty = 0, cex = 0.6)  # Adjust text size

mod_tree_pred = predict(mod_tree, newdata = testing_data)
mod_tree_rmse = sqrt(mean((testing_data$SalePrice - mod_tree_pred)^2))
mod_tree_rmse

cv_tree = cv.tree(mod_tree)
names(cv_tree)

cv_tree

with(cv_tree, plot(dev~size, type = 'b'))
```


From the summary of our model, we see that there are nine leaves and just four of our 22 predictors were used in the tree construction: overall quality, above ground living area, total basement area, and remodel date. This tells us already that these four variables may have a big impact on sale price of a house in Ames, Iowa — once again, these splits were based on node purity as well.

Something ironic is that three of these four variables are continuous, while the parent node, overall quality, is categorical (range from 1-10 and can only take discrete values). We see from the dendrogram that the first split comes on when overall quality is $<$ 7.5. From the tree, we see that, once again mostly entirely based on these four variables, an overall living quality rating of less than 6.5 generally corresponds to a lower above ground living area and further corresponds to a cheaper house than that with a quality rating above 8.5.

Finally with our deviance plot, we see that the variance and deviance will both decrease when the size of the tree gets larger, which is what we knew already. The deviance is still concerningly high, but so is our RMSE, so we will be complacent. With an RMSE of above 42,000, we know we will likely not be choosing CARTs as our final model, however.


```{r bagging with testing}
set.seed(123)

mod_bag = randomForest(SalePrice ~ ., data = training_data,  mtry = NCOL(training_data)-1, importance = TRUE)
mod_bag

plot(mod_bag)

mod_bag_pred = predict(mod_bag, newdata = testing_data)
mod_bag_rmse  = sqrt(mean((testing_data$SalePrice - mod_bag_pred)^2))
mod_bag_rmse

varImpPlot(mod_bag, cex = 0.6)
```

Moving on to bagging, we see that bagging attempts to use all 22 of our predictor variables, as it shows in the summary. This is already going to do better than our decision tree model because, despite the forseeable problem of overfitting, we know that bagging reduces variance and prevents overfitting anyways.

From the plot, we see the error rate decreasing logarithmically with respect to the number of trees present — remember, bagging and forests use multiple decision trees to come up with a better model.

In our predictor importance plots, we see that the percentage increase in MSE encompassed by some familiar variables is very high, particularly for overall quality, above ground living area, and total basement square footage. We further see that garage area certainly has an influence on MSE, followed by a few other variables, and ending with the Stone Brook neighborhood, which seems to have the lowest affect on MSE.

Very similarly for node purity, we can see which variables the most pure — this goes back to our Gini index — and see that overall quality takes the throne yet again. We know that this rating is so important in determining the price a house is sold at in Ames. Generally, we can likely use this variable, and many of these variables in particular, to determine a general housing price in any neighborhood in the United States, at the very least.

Because we are taking the multiple decision trees approach here, we see that the RMSE reduces substantially to just over 25,000 — our decisions are getting better.


```{r Random Forest with testing}

set.seed(123)
mod_rf = randomForest(SalePrice ~ ., data = training_data,  mtry = sqrt(NCOL(training_data)-1), importance = TRUE)
mod_rf

plot(mod_rf)

mod_rf_pred = predict(mod_rf, newdata = testing_data)
mod_rf_rmse  = sqrt(mean((testing_data$SalePrice - mod_rf_pred)^2))
mod_rf_rmse

varImpPlot(mod_rf, cex = 0.6)
varImpPlot(mod_bag, cex = 0.6)

library(rpart)
library(rpart.plot)

tree_model <- rpart(SalePrice ~ ., data = testing_data)
rpart.plot::rpart.plot(tree_model)

# Get the feature importance
importance_matrix <- randomForest::importance(mod_rf)

# Perform hierarchical clustering on feature importance
dendrogram <- hclust(dist(importance_matrix))

# Plot the dendrogram
plot(dendrogram, main = "Dendrogram of Feature Importance", xlab = "Features", ylab = "Distance")

cat("Tree RMSE:", mod_tree_rmse, "\n")
cat("Bagging RMSE:", mod_bag_rmse, "\n")
cat("Random Forest RMSE:", mod_rf_rmse)
```

The first interesting result we see here is that only five variables were tried at each split for random forests — this may be a cause for concern, but this and bagging will generally give us the best results, so we will hold onto that thought.

We acheive relatively similar results for our error plot, so I will avoid redundancy there. However, with our MSE and node purity plots, we see an abrupt break from normalcy — with increasing MSE, we see that more nodes have a higher impact on the increase. With node purity, we see that despite a dominance we saw in the bagging model with overall quality, other variables seem to be competing for high purity in RF. Living area, garage area, basement area, year built and first floor area all have bigger impacts it seems now. This is awfully interesting, but perhaps makes more sense why we see only close to five variables considered at each split.

Moving onto the horizontal dendrogram, we attempt to pick out the most important features from our already condensed list. This doesn't really give us any new information, but is simply another depiction of some of our better features, ranked.

We also give a fairly similar approach with our dendrogram — this only shows us one of our several decision trees we use in RF. Nothing really different here in our results.

We finally attempt to view the RMSE for all our tree-based models. Now, it is clear that random forest does the best.

# Conclusions

We opted against running a KNN model, although we had discussed this in our presentation, because we felt this would not perform as well as any of our tree-based modeling.

In the end, however, although the lasso model helped us determine our final modeling techniques by using our condensed dataset, we chose our random forest model.

Given our random forest model, with an RMSE of just under 24,000, we can say, in comparison to other models, that this one performs the best of any of the approaches we took. We believe that one can confidently say several of the variables identified as important can be used when actually buying a house anywhere, not just in Ames, Iowa. Of course, a foundational piece of stats is inference, and, although this is just one sample, this tells us a lot about people's decision-making. 

Websites like Zillow tell us a lot of these metrics, as it is very important to determine the overall quality rating from the current homeowners to make more accurate decisions. We see that overall quality, above-ground living area, garage area, and really most factors involving some sort of living space/area tend to have the largest impact on a house's sale price, which makes a lot of sense. Consumers are going to want a good amount of space for their families and cars and other amenities, so it certainly makes sense that as these areas increase, as does the selling price.

We thank you for going through this report, and hopefully this can help someone when buying a house down the road.
